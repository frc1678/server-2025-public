{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO chdir to server path regardless of computer\n",
    "os.chdir(\"/home/andyzh45/citrus/server\")\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import database\n",
    "import tba_communicator as tba\n",
    "import utils\n",
    "from pathlib import Path\n",
    "\n",
    "# Params to edit\n",
    "# =====================================\n",
    "DB_NAME = \"test2025capt\"\n",
    "READ_CLOUD = False\n",
    "# =====================================\n",
    "\n",
    "OUT_PATH = f\"data/data_audit/{DB_NAME}_data_audit.html\"\n",
    "Path(utils.create_abs_path(\"data/data_audit/\")).mkdir(exist_ok=True)\n",
    "\n",
    "EVENT_KEY = DB_NAME[4:] if \"test\" in DB_NAME else DB_NAME\n",
    "SCHEMA = utils.read_schema(\"schema/data_accuracy.yml\")\n",
    "\n",
    "db = database.BetterDatabase(DB_NAME, READ_CLOUD)\n",
    "raw_data = db.get_documents(\"data_accuracy\")\n",
    "data = pd.DataFrame(raw_data)\n",
    "\n",
    "TEAM_LIST = list(map(lambda doc: doc[\"team_number\"], db.get_documents(\"obj_team\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hist(var, title, data_to_graph=data, ax=None):\n",
    "\n",
    "    if not ax:\n",
    "        g = sns.histplot(data_to_graph, x=var, color=\"green\")\n",
    "    else:\n",
    "        g = sns.histplot(data_to_graph, x=var, color=\"green\", ax=ax)\n",
    "    g.set(\n",
    "        title=title,\n",
    "        xlabel=\"Point Difference (Error)\"\n",
    "        if \"point\" in var\n",
    "        else \"Game Piece Difference (Error)\",\n",
    "        ylabel=\"Number of Alliances\",\n",
    "    )\n",
    "    plt.suptitle(\n",
    "        f\"Avg. error: {round(data_to_graph[var].mean(), 1)} {'pts' if 'point' in var else 'pieces'}\",\n",
    "        fontsize=10,\n",
    "        y=0.85,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"total_point_diff\", \"Total Point Differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"total_piece_diff\", \"Total Gamepiece Differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"total_coral_diff\", \"Total Coral Count Differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"total_net_diff\", \"Total Net Count Differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\n",
    "    \"total_net_diff\",\n",
    "    \"Total Net Count Differences, Conditional on n > 0\",\n",
    "    data[data[\"total_net_diff\"] > 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"total_processor_diff\", \"Total Processor Count Differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\n",
    "    \"total_processor_diff\",\n",
    "    \"Total Processor Count Differences, Conditional on n > 0\",\n",
    "    data[data[\"total_processor_diff\"] > 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"auto_coral_diff\", \"Auto Coral Differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"tele_coral_diff\", \"Tele Coral Differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endgame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"endgame_point_diff\", \"Endgame Point Differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Error Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_team = {team: 0 for team in TEAM_LIST}\n",
    "for team in TEAM_LIST:\n",
    "    by_team[team] = np.mean(\n",
    "        list(\n",
    "            map(\n",
    "                lambda doc1: doc1[\"total_piece_diff\"],\n",
    "                filter(lambda doc: team in doc[\"team_numbers\"], raw_data),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "by_team_df = pd.DataFrame(list(by_team.items()), columns=[\"team_number\", \"diff\"])\n",
    "\n",
    "g = sns.barplot(\n",
    "    by_team_df.sort_values(\"diff\", ascending=False).iloc[:10],\n",
    "    x=\"team_number\",\n",
    "    y=\"diff\",\n",
    "    color=\"green\",\n",
    ")\n",
    "g.set(\n",
    "    title=\"Teams with Highest Avg. Errors\",\n",
    "    xlabel=\"Team Number\",\n",
    "    ylabel=\"Gamepiece Difference (Error)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_to_team_err = []\n",
    "\n",
    "for team in TEAM_LIST:\n",
    "    rating_to_team_err.append(\n",
    "        {\n",
    "            \"team_number\": team,\n",
    "            \"avg_diff\": by_team[team],\n",
    "            \"rating\": db.get_documents(\"pickability\", {\"team_number\": team})[0][\n",
    "                \"first_pickability\"\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    pd.DataFrame(rating_to_team_err), x=\"rating\", y=\"avg_diff\", color=\"green\"\n",
    ")\n",
    "g.set(\n",
    "    title=\"Team Avg. Error vs. Pickability\",\n",
    "    xlabel=\"First Pickability\",\n",
    "    ylabel=\"Avg. Error (pts)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_data = []\n",
    "\n",
    "for alliance in db.get_documents(\"predicted_aim\"):\n",
    "    alliance_data = db.get_documents(\n",
    "        \"data_accuracy\", {\"match_number\": alliance[\"match_number\"]}\n",
    "    )[0]\n",
    "    alliance_data.update(alliance)\n",
    "    score_data.append(alliance_data)\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    pd.DataFrame(score_data), x=\"actual_score\", y=\"total_piece_diff\", color=\"green\"\n",
    ")\n",
    "g.set(title=\"Match Error vs. Match Score\", xlabel=\"Match Score\", ylabel=\"Match Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scout Disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = db.get_documents(\"unconsolidated_totals\")\n",
    "tba_matches = tba.tba_request(f\"event/{EVENT_KEY}/matches\")\n",
    "\n",
    "disagreements = []\n",
    "for match_number in sorted(\n",
    "    list(map(lambda m: int(m), utils.get_match_schedule().keys()))\n",
    "):\n",
    "    match_sims = list(filter(lambda s: s[\"match_number\"] == match_number, sims))\n",
    "\n",
    "    for team in list(set([s[\"team_number\"] for s in match_sims])):\n",
    "        reported_values = []\n",
    "\n",
    "        for sim in list(filter(lambda s: s[\"team_number\"] == team, match_sims)):\n",
    "            reported_values.append(\n",
    "                utils.calc_weighted_sum(\n",
    "                    sim, SCHEMA[\"--diffs\"][\"total_piece_diff\"][\"tim_weights\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        max_ = max(reported_values)\n",
    "        min_ = min(reported_values)\n",
    "        disagreements.append(\n",
    "            {\n",
    "                \"match_number\": match_number,\n",
    "                \"team_number\": team,\n",
    "                \"reported_values\": reported_values,\n",
    "                \"min\": min_,\n",
    "                \"max\": max_,\n",
    "                \"range\": (max_ - min_),\n",
    "            }\n",
    "        )\n",
    "\n",
    "disagreements = pd.DataFrame(disagreements)\n",
    "\n",
    "g = sns.histplot(disagreements, x=\"range\", color=\"green\")\n",
    "g.set(\n",
    "    title=\"Scout Disagreements\",\n",
    "    xlabel=\"Range of Reported Values\",\n",
    "    ylabel=\"Number of Robots\",\n",
    ")\n",
    "plt.suptitle(\n",
    "    f\"Median disagreement: {round(disagreements['range'].median(), 1)} pieces\", y=0.85\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_team_df = (\n",
    "    disagreements[[\"team_number\", \"range\"]].groupby(\"team_number\").mean().reset_index()\n",
    ")\n",
    "\n",
    "g = sns.barplot(\n",
    "    by_team_df.sort_values(\"range\", ascending=False).iloc[:10],\n",
    "    x=\"team_number\",\n",
    "    y=\"range\",\n",
    "    color=\"green\",\n",
    ")\n",
    "g.set(\n",
    "    title=\"Teams with Highest Disagreements\",\n",
    "    xlabel=\"Team Number\",\n",
    "    ylabel=\"Average Disagreement\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors Across Alliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = list(\n",
    "    set(\n",
    "        (\n",
    "            map(\n",
    "                lambda t2: t2[\"match_number\"],\n",
    "                filter(\n",
    "                    lambda t: \"1678\" in t[\"team_numbers\"]\n",
    "                    or \"1323\" in t[\"team_numbers\"]\n",
    "                    or \"604\" in t[\"team_numbers\"],\n",
    "                    raw_data,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "top_match_data = pd.DataFrame(\n",
    "    list(\n",
    "        filter(\n",
    "            lambda t: t[\"match_number\"] in top_matches\n",
    "            and (\n",
    "                \"1678\" in t[\"team_numbers\"]\n",
    "                or \"1323\" in t[\"team_numbers\"]\n",
    "                or \"604\" in t[\"team_numbers\"]\n",
    "            ),\n",
    "            raw_data,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "opposing_data = pd.DataFrame(\n",
    "    list(\n",
    "        filter(\n",
    "            lambda t: t[\"match_number\"] in top_matches\n",
    "            and not (\n",
    "                \"1678\" in t[\"team_numbers\"]\n",
    "                or \"1323\" in t[\"team_numbers\"]\n",
    "                or \"604\" in t[\"team_numbers\"]\n",
    "            ),\n",
    "            raw_data,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "graph_hist(\n",
    "    \"total_piece_diff\",\n",
    "    \"Avg. Match Error of Alliances with 1678, 1323, or 604\",\n",
    "    top_match_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(\"total_piece_diff\", \"Avg. Match Error of Opposing Alliances\", opposing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = \"1678\"\n",
    "\n",
    "team_data = pd.DataFrame(list(filter(lambda t: team in t[\"team_numbers\"], raw_data)))\n",
    "\n",
    "g = sns.lineplot(team_data, x=\"match_number\", y=\"total_piece_diff\", color=\"green\")\n",
    "g.set(\n",
    "    title=f\"Team {team}'s Error Over Time\",\n",
    "    xlabel=\"Match Number\",\n",
    "    ylabel=\"Total Piece Difference (Error)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPR Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr_data = pd.DataFrame(db.get_documents(\"scout_precision\"))\n",
    "\n",
    "g = sns.histplot(spr_data, x=\"scout_precision\", color=\"green\")\n",
    "g.set(title=\"SPR Distribution\", xlabel=\"SPR\", ylabel=\"Number of Scouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.knit_ipynb(\"src/data_audit.ipynb\", OUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
